

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Hadoop</title>
    <meta name="description" content="Explanations to install and run Hadoop">
    <meta name="author" content="Arjen P. de Vries">

    <!-- New stuff, copied from http://www.w3schools.com/bootstrap -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

<!--  Use newer bootstrap? If so, go here:
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
-->
    <link href="https://rubigdata.github.io/course/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="https://rubigdata.github.io/course/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="https://rubigdata.github.io/course/assets/themes/twitter/css/kbroman.css" rel="stylesheet" type="text/css" media="all">
    <link href="https://rubigdata.github.io/course/assets/themes/twitter/css/arjenpdevries.css" rel="stylesheet" type="text/css" media="all">

    <!-- atom & rss feed -->
    <link href="https://rubigdata.github.io/coursenil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="https://rubigdata.github.io/coursenil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="https://rubigdata.github.io/course">RU Big Data 2019 (NWI-IBC036)</a>
        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        

<div class="page-header">
  <h2>Hadoop  <small>Running Hadoop 2.9.2 in your Docker container</small></h2>
</div>

<div class="row-fluid">
  <div class="span12">
    <h2 id="map-reduce-on-hdfs">Map Reduce on HDFS</h2>

<p>We have prepared an extended Spark-Notebook Docker container, prepared to run Hadoop on our own “pseudo-cluster”.
You will use the <code class="language-plaintext highlighter-rouge">docker exec</code> command to start a shell inside the Docker container, and work from that shell.</p>

<h3 id="setup">Setup</h3>

<h4 id="first-time-setup">First time setup</h4>

<p>Run the course’s Docker container and execute a shell, in which we still have to start the <code class="language-plaintext highlighter-rouge">ssh</code> service 
(a shortcoming of our current Docker image).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Create the container and start it
DID=$(docker create -p 50070:50070 -p 50075-50076:50075-50076 -p9000:9000 rubigdata/hadoop)
docker start $DID

## Execute a shell in the running container
docker exec -it $DID /bin/bash
</code></pre></div></div>

<p><em>Note:
The above command fails under Powershell (Windows) or, on a Linux laptop, if you did not setup the docker group correctly.
You can simply not use the <code class="language-plaintext highlighter-rouge">DID</code> environment variable (for Windows) or add <code class="language-plaintext highlighter-rouge">sudo</code> (on Linux) - the <code class="language-plaintext highlighter-rouge">sudo</code> inside the
<code class="language-plaintext highlighter-rouge">$()</code> though, not before <code class="language-plaintext highlighter-rouge">DID</code>!</em></p>

<p>The port mappings (<code class="language-plaintext highlighter-rouge">-p</code> flags on container creation) allow access to services that run inside the container, from a webbrowser 
that runs on the host. Port 50070 is the namenode WebUI, ports 50075 and 50076 provide datanode WebUIs.
Port 9001 is configured as the IPC port for the namenode, it does not need to be exposed.
<em>Port 9000 provides access to spark notebook, which we use in later sessions of the course; I included the mapping for
later re-use of the same instructions.</em></p>

<h4 id="returning-student">Returning student</h4>

<p>If you worked on a container before, you can continue with that same container instead of starting a new one. 
Docker containers can be started and stopped whenever you want!</p>

<p>Use <code class="language-plaintext highlighter-rouge">docker start</code> with the right hash; use <code class="language-plaintext highlighter-rouge">docker ps -a</code> to find it.</p>

<p><em>Of course, this assumes you work on the same terminal PC as previous week. Docker images and containers are stored locally on each machine, not in your homedir under NFS. (PS: If another student works on that PC under Linux, you can simply <code class="language-plaintext highlighter-rouge">ssh</code> into it!)</em></p>

<h3 id="intermezzo">Intermezzo</h3>

<p>If you are exploring the source of the Hadoop examples later on in the lab session, I recommend doing these steps 
on your <em>host machine</em>, and not inside the docker container; much easier for working with your favourite GUI, editors, 
copy-paste support, <em>etc. etc.</em></p>

<p>You can exchange files between the host and the container in three different ways:</p>

<ol>
  <li>
    <p>Use <code class="language-plaintext highlighter-rouge">docker cp</code> to copy files between a container and the local filesystem.</p>
  </li>
  <li>
    <p>Use <code class="language-plaintext highlighter-rouge">scp</code> to copy files via <code class="language-plaintext highlighter-rouge">lilo</code> (the FNWI LInux LOgin server); your homedir in the terminal room
is also mounted through NFS on <code class="language-plaintext highlighter-rouge">lilo</code>.</p>
  </li>
  <li>
    <p><strong>Third option, not possible in the Huygens terminal rooms:</strong> 
Create a directory <code class="language-plaintext highlighter-rouge">${HOME}/bigdata</code> in your homedir that you share with the Docker container using the <code class="language-plaintext highlighter-rouge">-v</code> 
flag to the <code class="language-plaintext highlighter-rouge">docker run</code> command.</p>
  </li>
</ol>

<h3 id="pseudo-distributed">Pseudo Distributed</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd hadoop-2.9.2
</code></pre></div></div>

<p>We will now setup a “real” cluster, even though we will only emulate it on our machine (<em>inside</em> the Docker container, actually).</p>

<p>You find the configuration files prepared as <code class="language-plaintext highlighter-rouge">etc/hadoop/core-site.xml</code> and <code class="language-plaintext highlighter-rouge">etc/hadoop/hdfs-site.xml</code>;
inspect them and you notice that replication has been set to one instead of the default of three 
<em>(Q: why does that make sense in the pseudo cluster scenario?!)</em>.</p>

<h4 id="hdfs">HDFS</h4>

<p>Prepare (format) the distributed filesystem:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bin/hdfs namenode -format
</code></pre></div></div>

<p>Start HDFS and create the user directory; here, I assume you simply work as user <code class="language-plaintext highlighter-rouge">root</code> in the Docker container.
(If you skipped “Setup passphraseless ssh” you will have to type <code class="language-plaintext highlighter-rouge">yes</code> a few times; once for every node in the pseudocluster that we create.)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbin/start-dfs.sh

bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/root

</code></pre></div></div>

<p>To run map-reduce jobs on a cluster, you first have to copy the data to the HDFS filesystem.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bin/hdfs dfs -put etc/hadoop input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar grep input output 'dfs[a-z.]+'
bin/hdfs dfs -get output output

bin/hdfs dfs -ls hdfs://localhost:9001/user/root/input
</code></pre></div></div>

<p>Try to understand exactly the effect of each of these commands; <em>what files are you doing what operation on?</em>
Extra hint: try <code class="language-plaintext highlighter-rouge">grep dfs.class etc/hadoop/*</code> and compare its results with the output you created above.</p>

<p>You can view the web interface for the namenode by opening <a href="http://localhost:50070/">localhost:50070/</a> in a browser on your host, because option <code class="language-plaintext highlighter-rouge">-p</code> on the <code class="language-plaintext highlighter-rouge">docker run</code> command above maps port 50070 inside the container (the namenode) onto the exact same port at the host (that you access in the browser). Note that the <code class="language-plaintext highlighter-rouge">localhost:9001</code> in the <code class="language-plaintext highlighter-rouge">hdfs dfs</code> command above refers to port 9001 inside the container!</p>

<p>When trying to figure out what happens exactly, it is good to know that the source of the example programs
is included in the release, e.g.,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jar tvf share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.9.2-sources.jar
</code></pre></div></div>

<p>(You can unpack <code class="language-plaintext highlighter-rouge">.jar</code> files (for “java archive”) using <code class="language-plaintext highlighter-rouge">jar xf</code>.)</p>

<p>Stop the filesystem gracefully when you are done:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbin/stop-dfs.sh
</code></pre></div></div>

<p>See also the Hadoop documentation for running a
<a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation">Pseudo-Distributed Cluster</a></p>

<h3 id="adding-a-second-datanode">Adding a second datanode</h3>

<p>If you complete the assignments easily, it can be interesting to see how to increase our (pseudo-distributed) cluster by ading a (virtual) datanode.
This part of the tutorial is optional - if you struggle through the previous, feel free to skip it.</p>

<p>Basically, follow instructions given in <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/201009.mbox/%3CA3EF3F6AF24E204B812D1D24CCC8D71A03688F76@mse16be2.mse16.exchange.ms%3E" title="Mailing list message">this mail from the hadoop mailing list</a>, updated for the current Hadoop version:</p>

<ol>
  <li>Copy <code class="language-plaintext highlighter-rouge">etc/hadoop</code> to <code class="language-plaintext highlighter-rouge">etc/hadoop2</code> and create a new data directory in <code class="language-plaintext highlighter-rouge">/tmp</code>, e.g., <code class="language-plaintext highlighter-rouge">mkdir /tmp/hadoop-root/dfs/data_02</code></li>
  <li>Edit <code class="language-plaintext highlighter-rouge">etc/hadoop2/hadoop-env.sh</code> to define a new datanode name, <code class="language-plaintext highlighter-rouge">export HADOOP_IDENT_STRING=${USER}_02</code></li>
  <li>Update <code class="language-plaintext highlighter-rouge">etc/hadoop2/hdfs-site.xml</code> with the additional parameters. You can also download <a href="/course/background/hdfs-site.xml">my version</a> and copy it into the container.</li>
  <li>Finally, we can start the second datanode: <code class="language-plaintext highlighter-rouge">bin/hdfs --config etc/hadoop2 datanode</code></li>
</ol>

<p>You can see the second datanode appear in the WebUI of the namenode, <a href="http://localnode:50070">localhost:50070</a>. 
Also, if you kill the datanode by pressing <code class="language-plaintext highlighter-rouge">^C</code>, eventually the namenode will notice and the second datanode marked red. Restart it, and it is green again.
You may want to play with configuring the cluster differently by modifying the replication factor to understand HDFS behaviour in more detail.</p>

<h3 id="see-also">See also</h3>

<p>Real clusters use a cluster management system like <em>Yarn</em> to start and stop services and manage map-reduce jobs.</p>

<p>If you are interested to see how that works (voluntarily, not required for the course), 
you could try the additional steps from the Hadoop documentation to
<a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node">run Yarn on a single node</a>,
keeping in mind that we use port 9001 where the documentation uses 9000.</p>

<p><a href="../assignments/A2-mapreduce.html">Back to Map-Reduce assignment</a></p>


  </div>
</div>


      </div>
      <hr>
      <footer>
        <p><small>
  <!-- start of footer -->
          <a href="https://arjenp.dev/">Arjen P. de Vries</a>
  <!-- end of footer -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>

