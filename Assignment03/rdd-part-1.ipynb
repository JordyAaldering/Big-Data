{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBE76734AF7B4C71875E497B740C990E"
   },
   "source": [
    "# Big Data: Spark RDD\n",
    "\n",
    "## Getting acquainted with Spark and Spark Notebook\n",
    "\n",
    " Never used a Notebook? \n",
    " Find useful advice in the UI tour (in the help menu) or in the \n",
    " [Spark Notebook documentation](https://github.com/spark-notebook/spark-notebook/blob/master/docs/exploring_notebook.md) itself.\n",
    " \n",
    " Take your time to practice using the Notebook environment, add new cells, split existing ones, switch between code and markdown, _etc. etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1C7F975861E47B5A264ABAE7D54B597"
   },
   "source": [
    "## Scala\n",
    "\n",
    "The first weeks of the course, you had the chance to try out some Scala with a special docker container. Now, you can return to the exercises you tried, but enter the scala in this notebook instead of using an editor and standalone scala compiler. Refer to the following resources for additional info on Scala:\n",
    "* Main [Scala site](http://scala-lang.org/), [tutorial](http://docs.scala-lang.org/tutorials/scala-for-java-programmers.html) and [API documentation](http://www.scala-lang.org/api/current/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "04BACA08063745B69B85A27FF64A3C58",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// A Scala expression for you to execute:\n",
    "\n",
    "1 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19F259914B0B4C1FAD16F9D80810990E",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab782323885-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "// Empty cell for you to try out some more Scala tests here!\n",
    "// Add a cell below to create more space for playing around with Scala.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5E4E1A5CCB0421887D57DC131629722"
   },
   "source": [
    "_Just in case:_ some Scala background is definitely useful to get things done, but __do not get carried away__, _this_ course is about big data processing, not about functional programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1C7F975861E47B5A264ABAE7D54B597"
   },
   "source": [
    "## Spark\n",
    "\n",
    "From now on, we consider Scala only as a __host language__ for the Spark big data platform. We access Spark from the host language through a special variable, the Spark Context, in these Spark Notebooks available as `sc`.\n",
    "\n",
    "The basic data structure in Spark is the __Resilient Distributed Dataset (RDD)__, that represents collections of items stored _in memory_ on many different computers in the data center (similar to files in Hadoop being represented by one or more blocks in the Hadoop distributed filesystem, RDDs consist of one or more so-called _partitions_ that may reside on different worker nodes).\n",
    "\n",
    "### Background information\n",
    "\n",
    "The following two links give (1) an introduction to using Spark's RDDs to represent collections and (2) the complete programming guide discussing all operations you can apply to RDDs (the latter as a reference to check for more detailed information).\n",
    "\n",
    "* http://spark.apache.org/examples.html\n",
    "* http://spark.apache.org/docs/2.3.2/programming-guide.html\n",
    "\n",
    "### My First RDD\n",
    "\n",
    "RDDs can be initiated from in-memory collections or from files in the (distributed or local) file system.\n",
    "\n",
    "Let's first initialize a new RDD from a collection of numbers created by Scala expression `0 to 999` using\n",
    "operation `parallelize` on the `Spark Context` \n",
    "([see the documentation](http://spark.apache.org/docs/2.3.2/api/scala/index.html#org.apache.spark.SparkContext)).\n",
    "The second parameter is optional, and instructs the platform to split the data in 8 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DA4E277FAF7C4B778E8ABB13B4B9AFB8",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(0 to 999,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BFD4BBEBE904EF68E25F4C7EA3BCA39"
   },
   "source": [
    "Evaluation of operations in Spark is lazy - only operations that require output to be materialized will actually trigger execution. Remember that evaluation is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened.\n",
    "\n",
    "_Check:_ Spark UI: [stages](http://localhost:4040/stages/) is still empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BE5337693CC44BF82E9ABCB48106FF4",
    "input_collapsed": false,
    "presentation": {
     "tabs_state": "{\n  \"tab_id\": \"#tab18227827-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "val sample = rdd.takeSample(false, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545599137D8E4CEB8844C430F48BA0CC"
   },
   "source": [
    "Only now, evaluation took place: see the [stages](http://localhost:4040/stages/) in the Spark UI.\n",
    "Click on the links!\n",
    "\n",
    "Try to explain for yourself: _Why would Spark have created two jobs?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24BABECC852D44758D23663C539DD7ED"
   },
   "source": [
    "### Data\n",
    "\n",
    "Use a shell escape to test if the Gutenberg data was correctly loaded on the docker container running the Spark Notebook.\n",
    "\n",
    "Note: [Assignment 2](http://rubigdata.github.io/course/assignments/A2-mapreduce.html) gave detailed instructions how to get the Project Gutenberg Shakespeare texts on your Spark Notebook container; download the data (again, if needed) and use `docker cp 100.txt snb:/opt/docker/hadoop-2.9.2` if your container is new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F206A4C70FF4C9680631DF621FB0CCE",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    ":sh ls /opt/docker/hadoop-2.9.2/100.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8195BB9B8624DFC80DADD10D7E6881E"
   },
   "source": [
    "### Counting words\n",
    "\n",
    "We will use the Shakespeare data for the classic Big Data \"Hello World\" exercise, counting words.\n",
    "\n",
    "_If you reached this part of the exercise before the first Spark lecture, feel free to continue, but definitely revisit this notebook **after** the lecture (you can clear previous output using the All Output in the Cell menu)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2338CDA0C3F743EE83C5FC5342E533F4",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"/opt/docker/hadoop-2.9.2/100.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B057B5D2E84548BCB735D89DD152C141"
   },
   "source": [
    "Can you predict what the following commands will do?\n",
    "Recognize the Map Reduce pattern on lines 2 and 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23D1AF9D2CE1411E8B6F999DC05F63D3",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "println( \"Lines:\\t\", lines.count, \"\\n\" + \n",
    "         \"Chars:\\t\", lines.map(s => s.length).\n",
    "                           reduce((a, b) => a + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00E7284A28DF42E7898D5F04431534D6"
   },
   "source": [
    "The map operator executes its parameter, the lambda function, on every item in the RDD.\n",
    "Reduce is also defined using a lambda function.\n",
    "\n",
    "_Note:_ if you never took a functional programming course, look at [this answer on StackExchange](http://stackoverflow.com/a/16509/2127435).\n",
    "\n",
    "Now try to understand in detail the following example.\n",
    "_Try to understand why we used `flatMap` and not `map`._\n",
    "\n",
    "It is worth copying the cell, and inspecting output at intermediate steps (use `take()`, not `collect()`.\n",
    "_After the first Spark lecture, in April, you should understand why!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0718A7BDEFFA4E5B8A5F6B1DB6BCBDE4",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "              .filter(_ != \"\")\n",
    "              .map(word => (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C10358C3D581449683E5E3EA737012F3",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val wc = words.reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "397206C23B6748F68657C4441680CB72",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab617899619-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "wc.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A77042B6782C45D395B30CA193D344EB"
   },
   "source": [
    "Take a look at how the platform processes this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1B893A39F6724E20BEE3715A68BC813E",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "wc.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F431CE5F87464C1A82606D456A883CE4"
   },
   "source": [
    "Inspect the Spark UI to see the computations in the cluster â†’ \n",
    "[see stages](http://localhost:4040/stages/) and their constituent tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B8904D8F4464B5D9EF451DE82482F98"
   },
   "source": [
    "### To count or not to count\n",
    "Ok, we can count words - let us find out which words Shakespeare used most often!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50F56A1566844BFB81CD15193D77C590",
    "input_collapsed": false,
    "presentation": {
     "tabs_state": "{\n  \"tab_id\": \"#tab758044651-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "val top10 = wc.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00F186A4DB7F410C93A01D8D4E6BD7C2"
   },
   "source": [
    "Ok, not quite what we wanted!\n",
    "See what's wrong?\n",
    "\n",
    "Let's fix the result ordering as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C32EE3BE19744CAF8752B47834966A9B",
    "input_collapsed": false,
    "presentation": {
     "tabs_state": "{\n  \"tab_id\": \"#tab1387326668-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "val top10 = wc.takeOrdered(10)(Ordering[Int].reverse.on(x=>x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CEFFF2741F045CC8326532D7C30239F"
   },
   "source": [
    "You can render the collected results however you want to using the client programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80879A64B623457183DBD741D1A4A034",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1459715444-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "top10.map({case(w,c) => \"Word '%s' occurs %d times\".format(w,c)}).map(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71C7F0526A97490788D2F6BF15C50026"
   },
   "source": [
    "We can zoom in on specific word frequencies, that might be more interesting than stopwords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "022554E0682B49429818282660C07E98",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1829423346-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "wc.filter(_._1 == \"Romeo\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "412B8D1CA6D24D7E86B4FC001D530F56",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1155248427-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "wc.filter(_._1 == \"Julia\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31E04AFEBE4B40B2A8D36536E7D3E491",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "wc.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82169E8C023C4E919C383CD2F6A6901D",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab222457634-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "wc.filter(_._1 == \"Macbeth\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E37D99D4857F457B802D4A49E4CA8582",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab407073836-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "wc.filter(_._1 == \"Capulet\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9AA7568D5EE45D785975A1131085309"
   },
   "source": [
    "Many different ways exist to compute the top N results. A few follow - _try to understand what actual work (for the cluster) is actually generated by the various alternatives._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAD7932FEAD345178AF23D96445985B3",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val oCounts = wc.map(x => x._2 -> x._1).sortByKey(false).map(x => x._2 -> x._1).cache()\n",
    "oCounts.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F8A955F7B064805886B1FCC255E7081",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// Alternative way to achieve the same:\n",
    "wc.sortBy(x => -x._2).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "465F5AADC3D647FC9030D3302BA05329",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// Preferred way if you really just want the top results\n",
    "// Note that you do not first need to assign the ordering function to a variable - you could just pass along the Ordering.by expression instead.\n",
    "val asc = (Ordering.by[(String, Int), Int](_._2))\n",
    "wc.top(10)(asc).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "50047376F71F48D098CCE2961195D5EE",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// Alternative formulation\n",
    "val desc = (Ordering.by[(String, Int), Int](-_._2))\n",
    "wc.takeOrdered(10)(desc).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8E29A81973A428586DAF1530FBCDDA7"
   },
   "source": [
    "The next section saves the results of word counting in the filesystem. \n",
    "\n",
    "We use a simple shell command to look into the directory that has been created.\n",
    "(Alternatively, you can navigate the filesystem after issuing a `docker exec -it HASH bash` command on the machine running the notebook container.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "85441E57359B42C2839BCCAF76482CBC",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A24D35BEA86A4F5B8566BFC83FC2449C",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    ":sh ls wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F87FDE9370F44F0E86781659B54ABFC9"
   },
   "source": [
    "_Q: Explain why there are multiple result files._\n",
    "\n",
    "Inspect the files from the command line in the docker container (using `docker exec`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5743005ED4344CC8ECA842A0EB81403"
   },
   "source": [
    "Clean up the directory to save headaches when later rerunning the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EFE613DC0ED14B5999023336084A7123",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    ":sh rm -rf wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FF056C361C74E14900145A8AA82A39E"
   },
   "source": [
    "### How to count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "940B8DC6670747C4B36B2EC22A1BA470",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "              .map(w => w.toLowerCase().replaceAll(\"(^[^a-z]+|[^a-z]+$)\", \"\"))\n",
    "              .filter(_ != \"\")\n",
    "              .map(w => (w,1))\n",
    "              .reduceByKey( _ + _ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "278814F8FBE749A7846C6BD58BA18FB9",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1918255847-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "words.filter(_._1 == \"macbeth\").collect\n",
    "  .map({case (w,c) => \"%s occurs %d times\".format(w,c)}).map(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89BBC292248E453C880A6C12DE59693C"
   },
   "source": [
    "_Q: why are the counts different?_"
   ]
  }
 ],
 "metadata": {
  "auto_save_timestamp": "1970-01-01T00:00:00.000Z",
  "customArgs": null,
  "customDeps": null,
  "customImports": null,
  "customLocalRepo": null,
  "customRepos": null,
  "customSparkConf": {
   "spark.app.name": "Notebook",
   "spark.executor.memory": "1G",
   "spark.master": "local[8]"
  },
  "customVars": null,
  "id": "0ac912f8-88a6-4fe9-9010-5082e92103d3",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "BigData-big-data-spark-rdd-part-1",
  "sparkNotebook": null,
  "trusted": true,
  "user_save_timestamp": "1970-01-01T00:00:00.000Z"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
