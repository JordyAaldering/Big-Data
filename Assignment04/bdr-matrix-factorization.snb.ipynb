{"metadata":{"id":"af087b8a-2f8d-4a28-bef9-c1f52d729c08","name":"bdr-matrix-factorization","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"sparkNotebook":null,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null,"customVars":null},"cells":[{"metadata":{"id":"1FD44A6FDA78453B8FE53F767F4C6910"},"cell_type":"markdown","source":"# Building a recommender with BigData Republic\n<img src=\"https://www.bigdatarepublic.nl/wp-content/uploads/2019/05/BDR_Logo_RGB_no_whitespace_small.jpg\" alt=\"BigData Republic\" style=\"width: 400px;\"/>\n\nThis notebook serves as an exercise to get familiar with recommender systems. In the big data industry, building a recommender is a very common use case so getting familiar with this is important both for aspiring data scientists and data engineers. Throughout this notebook we will show you how to process a real-world dataset with all its hurdles. Importantly, real-world data is often entirely different from pre-cleaned data you will find on e.g. Kaggle and requires a lot more preprocessing. Some of this preprocessing we have already done for you, but you will still need to do some preprocessing yourself before you can get started building your recommender. This notebook consists of three parts: preprocessing your data, building a recommender system using matrix factorization and evaluating your results.\n\n\n## The data\n<img src=\"https://truckstar.nl/app/uploads/2000/01/randstad-logo-share.png\" alt=\"Randstad\" style=\"width: 200px;\"/>\n\nWe have prepared a subset of a real-world dataset for you from _Randstad_, one of our clients. Randstad is the biggest Dutch employment agency that processes thousands of vacancies every month. They have agreed to share this data for educational purposes, as long as data is appropriately anonymized and subsetted. We therefore provide you with a dataset of vacancy/candidate combinations. Here, a combination can either mean that a candidate clicked on a vacancy, that a candidate started an application procedure or a candidate applied to a vacancy. Per combination, we also provide some extra information, such as the function description and the physical distance between the company and the candidate. This might be useful to increase the accuracy of your model. Furthermore, we provide a table with _profile data_. This contains some extra information on the candidates, such as their desired hourly wage and the amount of hours they want to work per week. This might also be useful to enrich your model later on.\n\nSince this data is shared with you privately and only for this course, **we ask you not to redistribute it**.\n\n## Getting the data\nFirst, we download the data from Amazon S3 storage and write it to a file. In total we're talking about approximately 300MB of data."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"FC30FB85591F483788EF1A78107F0133"},"cell_type":"code","source":["import sys.process._\n","import java.net.URL\n","import java.io.File\n","import scala.language.postfixOps\n","\n","new URL(\"https://s3-eu-west-1.amazonaws.com/bdr-college/profile_data.csv\") #> new File(\"notebooks/profile_data.csv\") !!\n","\n","new URL(\"https://s3-eu-west-1.amazonaws.com/bdr-college/click_data_train.csv\") #> new File(\"notebooks/click_data_train.csv\") !!\n","\n","new URL(\"https://s3-eu-west-1.amazonaws.com/bdr-college/click_data_val.csv\") #> new File(\"notebooks/click_data_val.csv\") !!\n","\n","new URL(\"https://s3-eu-west-1.amazonaws.com/bdr-college/zipcode_distances.csv\") #> new File(\"notebooks/zipcode_distances.csv\") !!\n","\n","new URL(\"https://s3-eu-west-1.amazonaws.com/bdr-college/vacancies_validation.csv\") #> new File(\"notebooks/vacancies_validation.csv\") !!"],"outputs":[]},{"metadata":{"id":"5F26BAC9A74640218B125842872B9016"},"cell_type":"markdown","source":"Next, we read that data into Spark DataFrames:"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"0F694F4E6AA14E878E5FDADAED1E0F81"},"cell_type":"code","source":["val spark = SparkSession\n","   .builder()\n","   .appName(\"BDRAssignment\")\n","   .getOrCreate()\n","\n","var profiles = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"notebooks/profile_data.csv\")\n","var clicks_train = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"notebooks/click_data_train.csv\")\n","var clicks_val = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"notebooks/click_data_val.csv\")\n","var zipcode_distances = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"notebooks/zipcode_distances.csv\")\n","var vacancies_val = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"notebooks/vacancies_validation.csv\")"],"outputs":[]},{"metadata":{"id":"6624B81CF2524C3980E7ADF0B6B4B8A6"},"cell_type":"markdown","source":"Since we have both clicks and applications from candidates, we can view these as ratings and add a weighting here to make applications weigh stronger than clicks. This usually gives better results. The \"ecom_action\" column in the clicks dataset determines the action of the candidate corresponding to that vacancy. 1 and 2 is a page view, 3 and 5 is starting an application procedure and a 6 is finishing an application. We use these values as weights for the model, however these initial values might be suboptimal. Experiment with different values for the optimal result. The values you choose as replacement directly influences the ratio of importance between a click and an application.\n\n### Assignment:\n* Convert all ones in the \"ecom_action\" column to a 2\n* Convert the 3's  and 5's  to 4's\n* Try to come up with a data-driven reasoning for choosing the ratio between clicks and applications"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"8B193BCA98D04995BB873E79D521E9E5"},"cell_type":"code","source":["clicks_val.groupBy(\"ecom_action\").count().show()"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"8BC8091C683A42A7AEAB6AF0BAEFC95C"},"cell_type":"code","source":["import org.apache.spark.sql.functions._\n","\n","// Hint: make a UDF that you can apply on a certain column in a dataframe"],"outputs":[]},{"metadata":{"id":"E3DF5D801FB9485484A016184BA4B168"},"cell_type":"markdown","source":"After modifying the \"ecom_action\" column we can now start training our recommender. For this we want to perform matrix factorization using the Alternative Least Squares algorithm. In Spark, we can do this using the org.apache.spark.ml.recommendation library. Use this library to perform ALS and make sure you understand the parameters that you need. There are some important choices that you will need to make, such as whether you want to use explicit or implicit matrix factorization and which values you want to use for regularization and the latent matrix rank. You can check out all these settings and more in the documentation: https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html.\n\n### Assignment:\n* Perform ALS on the clicks_train dataset\n* Check whether to use implicit or explicit matrix factorization\n* Look into the possible hyperparameters of the ALS function"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"BB128BA0F31949BD89A983D82F557507"},"cell_type":"code","source":["import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n","\n","// Hint: check out the documentation on ALS in Spark"],"outputs":[]},{"metadata":{"id":"4BF458103C074613A1627158F9DC5665"},"cell_type":"markdown","source":"You have now built your first recommender! However, there is a very important issue with the way we have set up this model. Think of the goal of this model. We can either try to recommend candidates to vacancies or vice versa, but vacancies are volatile! They come and go and are not always open to applications. To illustrate this, an example: let's say we've trained a recommender on data up until December 2017. There could exist a vacancy V_1 that was open in November 2017 which might be a great fit for a candidate C_1 looking for a job in May 2018, but we don't want to recommend him old vacancies because they are already closed. Of course, we can add a postprocessing filter to the output of the recommender to catch these erroneous recommendations, but still this won't fix the issue because there won't be any results left. This is caused by the recommender not being able to recommend vacancies that were not present in the training set (i.e. no vacancies that were posted after December 2017 will be recommended). Ergo, we need to find some way for the recommender to base its recommendations of NEW vacancies on the OLD vacancies. For this, we need the context of the vacancy, so that we can make comparisons between similar vacancies, instead of looking at each vacancy as a separate entity. We could, for example, look at the vacancy text to find common keywords. To keep it simple, we have added the \"function_name\" to each candidate/vacancy pair. This function name describes the category of the vacancy, and can be used to recommend job categories instead of specific vacancies. If we can recommend job categories (which can still be quite specific) we can later on search the most recent vacancies for that category and recommend those. This way, we solve the problem of not having any available data for recent vacancies.\n\n### Assignment:\n* Perform ALS on the function_name (item col) and candidate_number (user col)\n* ALS requires its inputs to be integers. Since the function_name is a string, you first need to convert this to an integer index. We've already done this for you.\n* There can be multiple rows for a single candidate/function_name pair. Make sure to aggregate the ratings (e.g. by summing the \"ecom_action\" column)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"50480AC72A6147E9B83A95D90E707B5E"},"cell_type":"code","source":["import org.apache.spark.ml.feature.StringIndexer\n","import org.apache.spark.ml.feature.IndexToString\n","import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n","\n","// We use a StringIndexer to convert strings to integers\n","val indexer = new StringIndexer()\n","  .setInputCol(\"function_name\")\n","  .setOutputCol(\"function_index\")\n","  .setHandleInvalid(\"skip\")\n","\n","// Fit the indexer\n","val stringIndexerModel = indexer.fit(clicks_train)\n","\n","// By transforming the stringindexer we get a new column 'function_index'\n","val clicks_train_indexed = stringIndexerModel.transform(clicks_train)\n","val clicks_val_indexed = stringIndexerModel.transform(clicks_val)\n","\n","// Hint: aggregate actions per candidate/function combination (e.g. group by the candidate number and function id and sum the rating per group)\n","val grouped_train = ...\n","val grouped_val = ...\n","\n","// Hint: run ALS in the same manner as you did before\n","// Don't make the rank and the number of iterations to high, it can cause memory issues\n","val als = ...\n","val model = ...\n","\n"],"outputs":[]},{"metadata":{"id":"EFF0545BB2C04A5E9CAB36C55E2ED11C"},"cell_type":"markdown","source":"## Sample predictions\nLets see if our recommender is producing expected results. We will look at a candidates clicking behavior and compare it with the output of the recommender for that candidate."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"28F816B0749143D78A00A1A1682A10B9"},"cell_type":"code","source":["// Take the first candidate\n","val candidate = clicks_train.select(\"candidate_number\").take(1)\n","\n","// See the clicks of the first candidate in the training set\n","val click_behavior = clicks_train.filter(col(\"candidate_number\") === candidate(0).getInt(0))\n","\n","click_behavior.show()"],"outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"C1DCCDE379D94952A1F22CDA7222905E"},"cell_type":"code","source":["// We need all the possible function ids in the training set\n","val function_ids = grouped_train.select(\"function_index\").distinct\n","\n","// Crossjoining the function ids with the candidate provides us with a dataframe that our model can use for prediction, \n","// For every candidate_number (in this case only the first one) and function_id, the model can calculate the predicted score\n","val candidate_functions = clicks_train.select(\"candidate_number\").limit(1).crossJoin(function_ids)\n","\n","// Predict the activations\n","val activations = model.transform(candidate_functions)\n","\n","// Invert the stringindexed results for verification purposes\n","val converter = new IndexToString()\n","  .setInputCol(\"function_index\")\n","  .setOutputCol(\"function_name\")\n","\n","// Convert the function indices to function names\n","val converted = converter.transform(activations)\n","\n","// Show the top predicted functions\n","converted.orderBy(desc(\"prediction\")).show()"],"outputs":[]},{"metadata":{"id":"12521C7CC0094CB8B78920D1F320AC15"},"cell_type":"markdown","source":"## Predict the validation set\nTesting recommender systems is a difficult task. A/B testing is usually the best option for the evaluation of a model. However we have no possibility if doing this in the timeframe of this exercise. To still get a general idea about the performance we will predict a ranking for vacancies in the validation set, and compare the real behavior of the candidates with our predictions. \n\nFirst we extract all unique candidates and unique function IDs in the validation set."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"D52B2CA87FF9432F8702350675E33643"},"cell_type":"code","source":["var candidate_numbers = clicks_val_indexed.select(\"candidate_number\").distinct\n","val function_ids = grouped_val.select(\"function_index\").distinct"],"outputs":[]},{"metadata":{"id":"81511BDFF914484DACC57E03351B41E5"},"cell_type":"markdown","source":"We want to predict a rating for every vacancy in the vacancies_val dataframe. In order to obtain the rating, we need to predict the job function score per candidate and optimize it using hourly wage, hours per week and distance as features. Lets take a look at the vacancies_val dataframe"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"3B1FC99387344F319782BB9EBA2EEDC9"},"cell_type":"code","source":["vacancies_val = stringIndexerModel.transform(vacancies_val)\n","vacancies_val show 20"],"outputs":[]},{"metadata":{"id":"500AC51BE77E42E88A3FA46F94F3FD83"},"cell_type":"markdown","source":"We are going to work with large dataframes, therefore we define a helper function take_top_n_grouped that selects the n highest records based on a sort column in the group."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E700D817FD854927BFFF0DAF63549CFD"},"cell_type":"code","source":["import org.apache.spark.sql.expressions.Window\n","import org.apache.spark.sql.functions.row_number\n","import org.apache.spark.sql.functions.{rank, desc}\n","\n","def take_top_n_grouped(df: DataFrame, n: Int, sortColumn: String, groupColumn: String) : DataFrame = {\n","\n","  // Window definition\n","  val w = Window.partitionBy(groupColumn).orderBy(desc(sortColumn))\n","\n","  // Filter\n","  val df_numbered = df.withColumn(\"rank\", row_number().over(w)).where($\"rank\" <= n)\n","  return df_numbered\n","}"],"outputs":[]},{"metadata":{"id":"9F53832C36C84B5D9986334C6F33F6B7"},"cell_type":"markdown","source":"For every unique candidate in the validation set we create their function predictions by joining the function ids with the candidates. The model transforms these pairs to an activation of which we select the top 5 activations per candidate."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"7CF5A52068474D5D9F6F1D8764B69581"},"cell_type":"code","source":["\n","// var candidates_limited = candidate_numbers.limit(10)\n","// Create all the candidate-function pairs\n","val validation = candidate_numbers.crossJoin(function_ids)\n","\n","// Predict the function ids score in the validation set for all users using the ALS model\n","var predictions = model.transform(validation)\n","\n","// For each candidate, select the top 5 job functions\n","predictions = take_top_n_grouped(predictions, n=5, sortColumn=\"prediction\", groupColumn=\"candidate_number\")\n"],"outputs":[]},{"metadata":{"id":"7E5062CBC26142BE8142FE2C8FADB35F"},"cell_type":"markdown","source":"To reduce the amount of vacancies we have to predict per candidate we filter the vacancies in the validation set by the top predicted functions per candidate. By joining these vacancies with the predictions we get new vacancy/candidate pairs. \nThere is one feature missing which is the distance for the candidate to the job. By joining with the profile information we can add the candidate zipcode to the vacancy/candidate pairs. \nBoth candidate and vacancy zipcodes are then in the dataframe. The distance is obtained by joining the dataframe with the zipcode_distances table."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"FBA8556A63584D8C8E1B5488F697F0D3"},"cell_type":"code","source":["// Add vacancies containing the correct function ids\n","val vacancies_filtered = vacancies_val.join(predictions, Seq(\"function_index\"), \"inner\")\n","\n","// We only need the candidate zipcode. Hint: you can try taking more columns (such as hourly wage) to incorporate additional features into the model.\n","val profiles_limited = profiles.select(\"candidate_number\", \"cand_pc\")\n","\n","// Add zipcode by joining on candidate number\n","val vacancies_zipcodes = vacancies_filtered.join(profiles_limited, Seq(\"candidate_number\"), \"left\")\n","\n","// Join on company postal code and candidate postal code\n","val vacancies_distance = vacancies_zipcodes.join(zipcode_distances, vacancies_zipcodes(\"company_pc\") === zipcode_distances(\"to\") && vacancies_zipcodes(\"cand_pc\") ===\n","                                           zipcode_distances(\"from\"), \"left\")"],"outputs":[]},{"metadata":{"id":"764B77E320E34C83870AB49B73CD2CEC"},"cell_type":"markdown","source":"All features are now present in our large table, however the values differ a lot between the features, therefore we need to normalize the distance, hour wage and function prediction between 0 and 1."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"4CF37A1ED6CD44C88B4D6C0F89219212"},"cell_type":"code","source":["// First calculate the maximum per candidate for each feature and create \n","val max_distances = vacancies_distance.groupBy(\"candidate_number\").agg(max($\"distance\").alias(\"max_distance\"), max($\"request_hour_wage\").alias(\"max_request_hour_wage\"),\n","                                                                 max($\"prediction\").alias(\"max_prediction\"))\n","\n","// Join the max distances in the vacancies table. Every candidate has now a unique maximum\n","var vacancies_max_joined = vacancies_distance.join(max_distances, Seq(\"candidate_number\"), \"inner\")\n","\n","// A lower distance is in this case better than a higher distance, therefore we subtract the normalized value from 1 after dividing by the maximum for that candidate\n","vacancies_max_joined = vacancies_max_joined.withColumn(\"normalized_distance\", lit(1) - col(\"distance\").divide(col(\"max_distance\")))\n","\n","// The other two features are divided by the maximum\n","vacancies_max_joined = vacancies_max_joined.withColumn(\"normalized_prediction\", col(\"prediction\").divide(col(\"max_prediction\")))\n","vacancies_max_joined = vacancies_max_joined.withColumn(\"normalized_request_hour_wage\", col(\"request_hour_wage\").divide(col(\"max_request_hour_wage\")))"],"outputs":[]},{"metadata":{"id":"1ECC1E5FE68E4937958A31713365BFA8"},"cell_type":"markdown","source":"With the normalized features we only have to combine them to create a prediction for that vacancy. The hour wage is weighted half, however these weights can be experimented with. For every candidate we select the 15 highest scored vacancies wich should appear on the first page if a candidate searched for vacancies on the Randstad site."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab2143038358-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"449264455C9644598ADED24DA284253A"},"cell_type":"code","source":["// Sum our three features. Hint: you can try playing with the multipliers of the features to see\n","// what works better. You can even do a grid search to really optimize your multipliers!\n","val scored_df = vacancies_max_joined.withColumn(\"total_score\", col(\"normalized_distance\") + col(\"normalized_prediction\") + lit(0.5) * col(\"normalized_request_hour_wage\"))\n","\n","// Take the top 15 per candidate\n","val top_n_scored_df = take_top_n_grouped(scored_df, n=15, sortColumn=\"total_score\", groupColumn=\"candidate_number\")"],"outputs":[]},{"metadata":{"id":"FF813D7C774741768D10F91B954CC31C"},"cell_type":"markdown","source":"## Evaluate the predictions\nThe last step is evaluating our predictions. We calculate the percentage of true applications that appeared in our predictions (recall)."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"F185C9733E084831AAB11AE0662DD34B"},"cell_type":"code","source":["// Select applications only\n","clicks_val = clicks_val.drop(\"action\")\n","val true_set = clicks_val.filter($\"ecom_action\" === 6)\n","\n","// Join our predictions with the true set, using \"inner\" join provides us with rows that appear in both sets.\n","val cross_set = true_set.join(top_n_scored_df, (top_n_scored_df(\"vacancy_number\")===true_set(\"vacancy_number\")) && (top_n_scored_df(\"candidate_number\")===true_set(\"candidate_number\")),\"inner\")\n","\n","val len_true = true_set.count()\n","val len_cross = cross_set.count()\n","if (len_true > 0) {\n","  // Simply use the lengths of the dataframes for our final score\n","  val final_percentage =  100 * len_cross / len_true.toDouble\n","  println(final_percentage.toString + \"% of the applications were in the prediction set\")\n","}\n","\n"],"outputs":[]},{"metadata":{"id":"3F9F41B476624E8887D8049EE191BE68"},"cell_type":"markdown","source":"## Compare with a baseline\nThe final percentage itself does not say much, is it high? low? To have a better feeling about this we need to compare it with a baseline score. In our case we select the top 15 most popular vacancies for every candidate. This baseline is better than selecting vacancies at random and provides a bit of insight."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"6B352AEA09D84276B2345F737C66DF4F"},"cell_type":"code","source":["val true_set = clicks_val.filter($\"ecom_action\" === 6)\n","// Select the 15 most applied vacancies\n","val popular_set = true_set.groupBy(\"vacancy_number\").count().orderBy(desc(\"count\")).limit(15)\n","// Sum the count\n","val pop_count = popular_set.agg(sum(\"count\")).first.getLong(0)\n","// The summed count is equal to the number of users that applied to the popular vacancies, dividing by the total gives us the recall.\n","val final_percentage =  100 * pop_count / true_set.count().toDouble\n","println(final_percentage.toString + \"% of the applications were in the prediction set\")"],"outputs":[]},{"metadata":{"id":"94196E04548A49219A905FC511F8691F"},"cell_type":"markdown","source":"## Start finetuning now!\nYou now have a working recommender and decent validation method. Try to improve performance by for instance:\n* Tuning feature weight parameters\n* Optimize ALS \n* Incorporate the profile data\n* Add week hours as feature\n* Filter out vacancies using rules (e.g. week hours don't fit the profile)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"F7C4A722C1314F4395094E8A48DC6ED4"},"cell_type":"code","source":[""],"outputs":[]}],"nbformat":4}